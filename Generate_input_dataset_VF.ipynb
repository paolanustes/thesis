{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Generate_input_dataset_VF.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3Vq5RIatn1uy"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paolanustes/thesis/blob/main/Generate_input_dataset_VF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfrk0JXlwgmT"
      },
      "source": [
        "# Input dataset pipeline for dams' detection models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWA8u4245eW9"
      },
      "source": [
        "# Import external resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj_QyTERxQgm"
      },
      "source": [
        "## Install and import main libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVy_yHpQ3yLr"
      },
      "source": [
        "References https://github.com/giswqs/geemap "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mqsnai4SgON"
      },
      "source": [
        "# !pip -q install qiskit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woxP8HcTC0--"
      },
      "source": [
        "!pip -q install -U geemap\n",
        "!pip -q install geopandas shapely"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA615jyCYE-z"
      },
      "source": [
        "import ee\n",
        "import geemap.eefolium as geemap\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF7PI-ZQRcJa"
      },
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #To access google drive folders\n",
        "GDRIVE='/content/drive/MyDrive/Thesis'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U8yKh_H5BRH"
      },
      "source": [
        "from shapely.geometry import Point\n",
        "from shapely.geometry import CAP_STYLE\n",
        "from pandas import json_normalize\n",
        "import requests\n",
        "import pandas as pd\n",
        "import geopandas\n",
        "import csv\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECKnyMyvw6wa"
      },
      "source": [
        "## Add Earth Engine data (Control group: California, USA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpwLPNNPSkwZ"
      },
      "source": [
        "1.   Satellite imagery NAIP https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/\n",
        "2.   Digital Elevation Model NED https://www.usgs.gov/core-science-systems/national-geospatial-program/national-map\n",
        "3. Boundaries by state TIGER https://www.census.gov/programs-surveys/geography/guidance/tiger-data-products-guide.html\n",
        "4. Water occurrence JRC http://global-surface-water.appspot.com/ \n",
        "5. Continent's boundaries users/gena/land_polygons_image \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y22sx-HAyFOq"
      },
      "source": [
        "#Create an interactive map\n",
        "Map = geemap.Map()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJsJSoggKcPY"
      },
      "source": [
        "#input number you want to search\n",
        "state = input('Enter state postal code: (Example for california enter CA)\\n')\n",
        "\n",
        "#read csv, and split on \",\" the line\n",
        "csv_file = csv.reader(open(f'{GDRIVE}/FIPS.csv', \"r\"), delimiter=\",\")\n",
        "\n",
        "\n",
        "#loop through the csv list\n",
        "for row in csv_file:\n",
        "    #if current rows 2nd value is equal to input, print that row\n",
        "    if state == row[1]:\n",
        "         print (row[2])\n",
        "         FIP = row[2]\n",
        "         S_NAME = row[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ7imnalL5sn"
      },
      "source": [
        "#Add control polygon of California //  https://datadrivenlab.org/big-data-2/google-earth-engine-tutorial/\n",
        "# Load US county dataset.\n",
        "countyData = ee.FeatureCollection('TIGER/2018/Counties').filter(ee.Filter.eq('STATEFP', FIP))\n",
        "\n",
        "# Get the union of the county geometries in the county.\n",
        "control = countyData.union(100).geometry().bounds(100)\n",
        "\n",
        "\n",
        "#Add Earth Engine dataset (NAIP)\n",
        "naip = ee.ImageCollection(\"USDA/NAIP/DOQQ\").filterDate('2016-01-01','2019-01-01')\n",
        "naip1= naip.median().clip(control);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uHCzGRHlvub"
      },
      "source": [
        "#Add water ocurrence map to see the water bodies\n",
        "water = ee.Image(\"JRC/GSW1_2/GlobalSurfaceWater\").select('occurrence').clip(control)\n",
        "water = water\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61j-XKwyala1"
      },
      "source": [
        "#Add DEM\n",
        "DEM = ee.Image(\"USGS/NED\").clip(control)\n",
        "aspect = ee.Terrain.aspect(DEM)\n",
        "slope  =ee.Terrain.slope(DEM)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVQQGaKKnfTE"
      },
      "source": [
        "#Add continents' boundaries to delete polygons on the coastline\n",
        "continents = ee.Image(\"users/gena/land_polygons_image\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stVAzCdTbKS6"
      },
      "source": [
        "# https://developers.google.com/earth-engine/guides/image_visualization \n",
        "\n",
        "ndwi = naip1.normalizedDifference(['G','N']).rename(['ndwi'])\n",
        "\n",
        "# To mask the non-watery parts of the image, where NDWI < 0.3\n",
        "ndwiMasked = ndwi.updateMask(ndwi.gte(0.5))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jWP2bV8VLqJ"
      },
      "source": [
        "## Normalized channels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPoZljV0oFfq"
      },
      "source": [
        "SCALE = 10\n",
        "DEMminMax = DEM.reduceRegion(\n",
        "  reducer= ee.Reducer.minMax(),\n",
        "  geometry= DEM.geometry(),\n",
        "  scale= SCALE,\n",
        "  maxPixels= 10e12\n",
        "  )\n",
        "\n",
        "ndwiminMax = ndwi.reduceRegion(\n",
        "  reducer= ee.Reducer.minMax(),\n",
        "  geometry= ndwi.geometry(),\n",
        "  scale= SCALE,\n",
        "  maxPixels= 10e12\n",
        "  )\n",
        "\n",
        "waterminMax = water.reduceRegion(\n",
        "  reducer= ee.Reducer.minMax(),\n",
        "  geometry= water.geometry(),\n",
        "  scale= SCALE,\n",
        "  maxPixels= 10e12\n",
        "  )\n",
        "\n",
        "AspectminMax = aspect.reduceRegion(\n",
        "  reducer= ee.Reducer.minMax(),\n",
        "  geometry= aspect.geometry(),\n",
        "  scale= SCALE,\n",
        "  maxPixels= 10e12\n",
        "  )\n",
        "\n",
        "SlopeminMax = slope.reduceRegion(\n",
        "  reducer= ee.Reducer.minMax(),\n",
        "  geometry= slope.geometry(),\n",
        "  scale= SCALE,\n",
        "  maxPixels= 10e12\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQhjegN0ikNT"
      },
      "source": [
        "norm_DEM = DEM.unitScale(ee.Number(DEMminMax.get('elevation_min')), ee.Number(DEMminMax.get('elevation_max'))).double()\n",
        "norm_ndwi = ndwi.unitScale(ee.Number(ndwiminMax.get('ndwi_min')), ee.Number(ndwiminMax.get('ndwi_max'))).double()\n",
        "norm_water = water.unitScale(ee.Number(waterminMax.get('occurrence_min')), ee.Number(waterminMax.get('occurrence_max'))).double() \n",
        "norm_aspect = aspect.unitScale(ee.Number(AspectminMax.get('aspect_min')), ee.Number(AspectminMax.get('aspect_max'))).double() \n",
        "norm_slope = slope.unitScale(ee.Number(SlopeminMax.get('slope_min')), ee.Number(SlopeminMax.get('slope_max'))).double() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYGFmbssDMV-"
      },
      "source": [
        "#To add the normalized channels as bands in the NAIP image\n",
        "naip2 = naip1.addBands(norm_ndwi).addBands(norm_water).addBands(norm_aspect).addBands(norm_slope).addBands(norm_DEM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgnMhrbI-2ZV"
      },
      "source": [
        "## Map visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n55bJalgxodN"
      },
      "source": [
        "# # Displays the map\n",
        "# #colors https://en.wikipedia.org/wiki/Web_colors \n",
        "# #https://github.com/gee-community/ee-palettes\n",
        "\n",
        "Map = geemap.Map()\n",
        "Map.setCenter(-120.08, 36.86)\n",
        "\n",
        "Map.addLayer(naip1, {}, \"NAIP\");\n",
        "\n",
        "#AOI: Area of interest\n",
        "Map.addLayer(control, {'color': 'yellow', 'opacity': 0.5}, \"AOI\");\n",
        "\n",
        "#DEM bands\n",
        "hs = ee.Terrain.hillshade(DEM)\n",
        "Map.addLayer(hs, {}, 'Elevation hillshade')\n",
        "Map.addLayer(aspect, {'min': 0, 'max': 360}, 'Aspect')\n",
        "Map.addLayer(slope, {}, 'Slope')\n",
        "\n",
        "#Water occurrence\n",
        "Map.addLayer(water, { 'min': 0, 'max': 1, 'palette': ['00ffff'] }, 'water')\n",
        "\n",
        "#NDWI\n",
        "Map.addLayer(ndwi, {'min': 0.5, 'max': 1, 'palette': ['00FFFF', '0000FF']}, 'NDWI')\n",
        "# ['0000ff', '00ffff', 'ffff00', 'ff0000', 'ffffff']\n",
        "Map.addLayer(ndwiMasked, {'min': 0.5, 'max': 1, 'palette': ['00FFFF', '0000FF']}, 'NDWI masked')\n",
        "\n",
        "Map.addLayerControl() \n",
        "Map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBVYsvJox2gv"
      },
      "source": [
        "# Collect dams' dataset from OpenStreetMap API and NID\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPDZCfnHmf3E"
      },
      "source": [
        "## Importing dams locations from OpenStreetMap (Overpass API)\n",
        "OSM API allows the extraction of data in form of features (node, way, relation). In the following algorithm is carried out the extraction of point locations classified as dams in OSM. \n",
        "\n",
        "To create the dam's queries with the right tags: https://taginfo.openstreetmap.org/tags\n",
        "\n",
        "*Note: the extraction of each type of feature has to be done separetely, otherwise the code selects just one type and the other features are not extracted properly.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_vPYJRpl_7r"
      },
      "source": [
        "url = 'http://overpass-api.de/api/interpreter'  # Overpass API URL\n",
        "query = f\"\"\"\n",
        "[out:json];\n",
        "area[\"ISO3166-2\"=\"US-{state}\"][admin_level=4];\n",
        "(node[\"waterway\"=\"dam\"](area);\n",
        ");\n",
        "out center;\n",
        "\"\"\"\n",
        "\n",
        "r = requests.get(url, params={'data': query})\n",
        "data = r.json()['elements']  # read response as JSON and get the data\n",
        "df = json_normalize(data).rename(columns={\"tags.name\":\"tag_name\"}) #creates a dataframe with the data extracted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybPRjXRWyFKP"
      },
      "source": [
        "url = 'http://overpass-api.de/api/interpreter'  # Overpass API URL\n",
        "query2 = f\"\"\"\n",
        "[out:json];\n",
        "area[\"ISO3166-2\"=\"US-{state}\"][admin_level=4];\n",
        "( way[\"waterway\"=\"dam\"](area);\n",
        ");\n",
        "out center;\n",
        "\"\"\"\n",
        "\n",
        "r2 = requests.get(url, params={'data': query2})\n",
        "data2 = r2.json()['elements']  # read response as JSON and get the data\n",
        "df2 = json_normalize(data2).rename(columns={\"center.lat\":\"lat\", \"center.lon\":\"lon\", \"tags.name\":\"tag_name\"})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiYAY7sFPQZ6"
      },
      "source": [
        "**Re-run the code below, sometimes presents error on the first run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoHz6Dn-RJ3f"
      },
      "source": [
        "url = 'http://overpass-api.de/api/interpreter'  # Overpass API URL\n",
        "query3 = f\"\"\"\n",
        "[out:json];\n",
        "area[\"ISO3166-2\"=\"US-{state}\"][admin_level=4];\n",
        "( rel[\"waterway\"=\"dam\"](area);\n",
        ");\n",
        "out center;\n",
        "\"\"\"\n",
        "\n",
        "r3 = requests.get(url, params={'data': query3})\n",
        "data3 = r3.json()['elements']  # read response as JSON and get the data\n",
        "df3 = json_normalize(data3).rename(columns={\"center.lat\":\"lat\", \"center.lon\":\"lon\", \"tags.name\":\"tag_name\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5jPBMvtgcKL"
      },
      "source": [
        "#Select the columns of the dataframe\n",
        "nodes = pd.DataFrame(df, columns=['lat','lon', 'type', 'tag_name'])\n",
        "ways = pd.DataFrame(df2, columns=['lat','lon', 'type', 'tag_name'])\n",
        "relations = pd.DataFrame(df3, columns=['lat','lon', 'type', 'tag_name'])\n",
        "\n",
        "#Append the three features and add index\n",
        "OSM_dams = nodes.append(ways).append(relations)\n",
        "OSM_dams.reset_index(drop = True, inplace= True)\n",
        "\n",
        "#Create GeoDataframe of the OSM feature collection\n",
        "gdf_OSM = geopandas.GeoDataFrame(OSM_dams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaN-I1LcR4T9"
      },
      "source": [
        "## Importing dams locations from National Inventory of Dams (NID) database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnzJ1-qsMnOs"
      },
      "source": [
        "lines = list()\n",
        "\n",
        "with open(f'{GDRIVE}/NID.csv', 'r') as readFile:\n",
        "\n",
        "    reader = csv.reader(readFile)\n",
        "\n",
        "    for row in reader:\n",
        "\n",
        "        for field in row:\n",
        "\n",
        "            if field == state:\n",
        "\n",
        "                lines.append(row)\n",
        "\n",
        "with open('mycsv.csv', 'w') as writeFile:\n",
        "\n",
        "    writer = csv.writer(writeFile)\n",
        "\n",
        "    writer.writerows(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b52mGvluDM9-"
      },
      "source": [
        "data_NID = pd.DataFrame(lines, columns=['RECORDID','LONGITUDE','LATITUDE',\n",
        "                                        'DAM_TYPE','PURPOSES','DAM_LENGTH','DAM_HEIGHT',\n",
        "                                        'MAX_STORAGE','SURFACE_AREA','HAZARD','STATE'])\n",
        "data_NID['LATITUDE']=pd.to_numeric(data_NID['LATITUDE'])\n",
        "data_NID['LONGITUDE']=pd.to_numeric(data_NID['LONGITUDE'])\n",
        "data_NID[\"id\"] = data_NID.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KojGJTKYEPQo"
      },
      "source": [
        "#Create GeoDataframe of the NID feature collection\n",
        "gdf_NID=geopandas.GeoDataFrame(data_NID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cxb9PezYjLc"
      },
      "source": [
        "grand = list()\n",
        "\n",
        "with open(f'{GDRIVE}/GRAND.csv', 'r') as readFile:\n",
        "\n",
        "    reader = csv.reader(readFile)\n",
        "\n",
        "    for row in reader:\n",
        "\n",
        "        for field in row:\n",
        "\n",
        "            if field == S_NAME:\n",
        "\n",
        "                grand.append(row)\n",
        "\n",
        "with open('mycsv.csv', 'w') as writeFile:\n",
        "\n",
        "    writer = csv.writer(writeFile)\n",
        "\n",
        "    writer.writerows(grand)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJTwPaFubhjf"
      },
      "source": [
        "data_GRAND = pd.DataFrame(grand, columns=['GRAND', 'NAME', 'RIVER', 'STATE', 'YEAR', \n",
        "                                          'HEIGHT', 'LEN', 'AREA_SKM', 'CAP_MCM', \n",
        "                                          'DIS_AVG_LS', 'MAIN_USE', 'QUALITY', 'LONG', 'LAT'])\n",
        "data_GRAND['LAT']=pd.to_numeric(data_GRAND['LAT'])\n",
        "data_GRAND['LONG']=pd.to_numeric(data_GRAND['LONG'])\n",
        "data_GRAND[\"id\"] = data_GRAND.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pGISh81coiY"
      },
      "source": [
        "#Create GeoDataframe of the NID feature collection\n",
        "gdf_GRAND=geopandas.GeoDataFrame(data_GRAND)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M40ORqSFPTX"
      },
      "source": [
        "## Create earth engine objects to add to the interactive map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4E0uJbjj3I2"
      },
      "source": [
        "#To convert the dataframe to shape to then create an earth engine object and add the layer to the map\n",
        "\n",
        "gdf_OSM.set_geometry(\n",
        "    geopandas.points_from_xy(gdf_OSM['lon'], gdf_OSM['lat']),\n",
        "    inplace=True, crs='EPSG:4326')\n",
        "gdf_OSM.drop(['lat', 'lon'], axis=1, inplace=True)  # optional\n",
        "gdf_OSM.to_file('dams_OSM.shp')\n",
        "\n",
        "ee_OSM = geemap.shp_to_ee('dams_OSM.shp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHFD3PEHqa_2"
      },
      "source": [
        "gdf_NID.set_geometry(\n",
        "    geopandas.points_from_xy(gdf_NID['LONGITUDE'], gdf_NID['LATITUDE']),\n",
        "    inplace=True, crs='EPSG:4326')\n",
        "gdf_NID.drop(['LATITUDE', 'LONGITUDE'], axis=1, inplace=True)  # optional\n",
        "\n",
        "gdf_NID[~gdf_NID.geometry.is_valid]\n",
        "gdf_NID = gdf_NID[gdf_NID.geometry.is_valid == True]\n",
        "gdf_NID.to_file('dams_NID.shp')\n",
        "\n",
        "ee_NID = geemap.shp_to_ee('dams_NID.shp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSmVVNOEc0pO"
      },
      "source": [
        "gdf_GRAND.set_geometry(\n",
        "    geopandas.points_from_xy(gdf_GRAND['LONG'], gdf_GRAND['LAT']),\n",
        "    inplace=True, crs='EPSG:4326')\n",
        "gdf_GRAND.drop(['LAT', 'LONG'], axis=1, inplace=True)  # optional\n",
        "\n",
        "gdf_GRAND[~gdf_GRAND.geometry.is_valid]\n",
        "gdf_GRAND = gdf_GRAND[gdf_GRAND.geometry.is_valid == True]\n",
        "gdf_GRAND.to_file('dams_GRAND.shp')\n",
        "\n",
        "ee_GRAND = geemap.shp_to_ee('dams_GRAND.shp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVzUC5RFTxcs"
      },
      "source": [
        "## Create a polygon with centroid in the imported feature collection (OSM/NID)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfeTLXI-6Ky3"
      },
      "source": [
        "buffer = gdf_OSM.buffer(0.01, cap_style=3) #0.02 is the dimension in lat/long and cap_style: 1 (round), 2 (flat), 3 (square)\n",
        "buffer_NID = gdf_NID.buffer(0.01, cap_style=3)\n",
        "buffer_GRAND = gdf_GRAND.buffer(0.01, cap_style=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg8uM-Zy5e3w"
      },
      "source": [
        "#From Geoseries to EE object\n",
        "buf_OSM = geopandas.GeoDataFrame(geometry=geopandas.GeoSeries(buffer))\n",
        "buf_OSM.to_file('buffer_OSMdams.shp')\n",
        "ee_polygon_OSM = geemap.shp_to_ee('buffer_OSMdams.shp')\n",
        "\n",
        "buf_NID = geopandas.GeoDataFrame(geometry=geopandas.GeoSeries(buffer_NID))\n",
        "buf_NID.to_file('buffer_NIDdams.shp')\n",
        "ee_polygon_NID = geemap.shp_to_ee('buffer_NIDdams.shp')\n",
        "\n",
        "buf_GRAND = geopandas.GeoDataFrame(geometry=geopandas.GeoSeries(buffer_GRAND))\n",
        "buf_GRAND.to_file('buffer_GRANDdams.shp')\n",
        "ee_polygon_GRAND = geemap.shp_to_ee('buffer_GRANDdams.shp')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4MTl44QSRF4"
      },
      "source": [
        "## Map visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJVRh42oxwW9"
      },
      "source": [
        "# Displays the map\n",
        "Map = geemap.Map()\n",
        "Map.setCenter(-120.08, 36.86)\n",
        "bounds = Map.get_bounds()\n",
        "\n",
        "Map.addLayer(naip1, {}, \"NAIP\");\n",
        "\n",
        "#Add points of OSM and NID feature collections\n",
        "Map.addLayer(ee_OSM, {'color': 'yellow'}, 'Dams OSM')\n",
        "Map.addLayer(ee_NID, {'color': 'lime'}, 'Dams NID')\n",
        "Map.addLayer(ee_GRAND, {'color': 'orange'}, 'Dams GRAND')\n",
        "\n",
        "#Add unfiltered polygons of OSM and NID dams\n",
        "Map.addLayer(ee_polygon_OSM, {'color': 'yellow', 'opacity': 0.5}, 'Polygon_dams_OSM')\n",
        "Map.addLayer(ee_polygon_NID, {'color': 'lime', 'opacity': 0.5}, 'Polygon_dams_NID')\n",
        "Map.addLayer(ee_polygon_GRAND, {'color': 'orange', 'opacity': 0.5}, 'Polygon_dams_GRAND')\n",
        "\n",
        "Map.addLayerControl() \n",
        "Map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK2JUwe8lxii"
      },
      "source": [
        "# Refine the dam dataset \n",
        "\n",
        "Remove data that can be noisy "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuHKt9-mm2aa"
      },
      "source": [
        "## Filtering functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLKnizRzm7A4"
      },
      "source": [
        "def compute_water_area(f):\n",
        "    water2 = water\n",
        "    Waterarea = ee.Image.pixelArea().mask(water2).reduceRegion(\n",
        "      reducer= ee.Reducer.sum(), \n",
        "      geometry= f.geometry(), \n",
        "      scale= SCALE)\n",
        "  \n",
        "    return f.set(Waterarea)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWZiJWChV3BD"
      },
      "source": [
        "#Function to verify point-in-polygon (pip)\n",
        "#Adapted from: https://medium.com/analytics-vidhya/point-in-polygon-analysis-using-python-geopandas-27ea67888bff \n",
        "\n",
        "def get_polygon_in_polygon (gdf, polygon):\n",
        "    id_list = list(polygon.FID)\n",
        "    #create empty dataframe\n",
        "    df = pd.DataFrame().reindex_like(gdf).dropna()\n",
        "    for i in id_list:\n",
        "        #get geometry for specific region\n",
        "        pol = (polygon.loc[polygon.FID==i])\n",
        "        pol.reset_index(drop = True, inplace = True)\n",
        "        #identify those records from gdf that are intersecting with the region polygon\n",
        "        pip_mask = gdf.intersects(pol.loc[0, 'geometry'])\n",
        "        #filter gdf to keep only the intersecting records\n",
        "        pip_data = gdf.loc[pip_mask].copy()\n",
        "        #create a new column and assign the FID as the value\n",
        "        pip_data['FID']= i\n",
        "        #append region data to empty dataframe\n",
        "        df = df.append(pip_data)\n",
        "        \n",
        "    #checking there are no more than one region assigned to a point  \n",
        "    print('Original dataframe count=',len(gdf),'\\nNew dataframe count=', len(df))\n",
        "    if df.loc[df.id.duplicated() == True].shape[0] > 0:\n",
        "        print(\"There are id's with more than one region\")\n",
        "\n",
        "    #checking all points have a region\n",
        "    elif gdf.loc[~gdf.id.isin(df.id)].shape[0] > 0:\n",
        "        print(\"There are id's without an assigned region\")\n",
        "    else:\n",
        "        print(\"No discrepancies in results!\")\n",
        "\n",
        "    df.reset_index(inplace=True)\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWdHX5Fxnsce"
      },
      "source": [
        "def in_land(f):\n",
        "  isInland = continents.mask().reduceRegion(\n",
        "      reducer= ee.Reducer.allNonZero(),\n",
        "      geometry= f.geometry(),\n",
        "      scale= 30).values().get(0)\n",
        "\n",
        "  return f.set({'isInland': isInland})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZPU6lRLR1BY"
      },
      "source": [
        "## Double validate location and remove remaining patches\n",
        "Since there are many overlapping patches from the two datasets imported (OSM and NID) in this section are filtered the overlapping patches and are removed the remaining patches. In this way, the location is double confirmed and it is reduce the risk of having non-dams included in the final dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IrY7fqtSYY0"
      },
      "source": [
        "buf_OSM['FID'] = buf_OSM.index\n",
        "buf_NID['FID'] = buf_NID.index+len(buf_OSM)\n",
        "buf_GRAND['FID'] = buf_GRAND.index+(len(buf_OSM)+len(buf_NID))\n",
        "buf_OSM['id'] = buf_OSM.index\n",
        "buf_NID['id'] = buf_NID.index+len(buf_OSM)\n",
        "buf_GRAND['id'] = buf_GRAND.index+(len(buf_OSM)+len(buf_NID))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxY8i1ARUjOg"
      },
      "source": [
        "overlay_polygons = get_polygon_in_polygon(buf_NID, buf_OSM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l8S2Pz2Vhl4"
      },
      "source": [
        "overlay_polygons.drop_duplicates('id', inplace= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG_QL789SfjG"
      },
      "source": [
        "# #From geodataframe to GEE features\n",
        "gdf_all = geopandas.GeoDataFrame(overlay_polygons)\n",
        "gdf_all.to_file('all_NID.shp')\n",
        "ee_all = geemap.shp_to_ee('all_NID.shp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52vM4Cv0TyXp"
      },
      "source": [
        "# # Displays the map\n",
        "Map = geemap.Map()\n",
        "Map.setCenter(-120.08, 36.86)\n",
        "\n",
        "Map.addLayer(naip1, {}, \"NAIP\");\n",
        "\n",
        "#Add unfiltered polygons of OSM and NID dams\n",
        "Map.addLayer(ee_polygon_OSM, {'color': 'yellow', 'opacity': 0.5}, 'Polygon_dams_OSM')\n",
        "Map.addLayer(ee_polygon_NID, {'color': 'lime', 'opacity': 0.5}, 'Polygon_dams_NID')\n",
        "Map.addLayer(ee_polygon_GRAND, {'color': 'orange', 'opacity': 0.5}, 'Polygon_dams_GRAND')\n",
        "\n",
        "Map.addLayer(ee_all, {'color': 'blue', 'opacity': 0.5}, 'Polygon_dams')\n",
        "\n",
        "Map.addLayerControl() \n",
        "Map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb-6sy7AnvrA"
      },
      "source": [
        "## Filter dam polygons by water area"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRD8UqH-n8aM"
      },
      "source": [
        "### Now we filter the squares for which area does not reach a certain threshold\n",
        "ee_dams_area = ee_all.map(compute_water_area)\n",
        "\n",
        "AreaThreshold = 80000\n",
        "ee_dams_water_filter = ee_dams_area.filter(ee.Filter.gt('area', AreaThreshold))\n",
        "\n",
        "print('All polygons OSM #{}'.format(ee_dams_area.size().getInfo()))\n",
        "print('Filtered polygons OSM by area #{}'.format(ee_dams_water_filter.size().getInfo()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oMlRRYRKZrO"
      },
      "source": [
        "# Displays the map\n",
        "Map = geemap.Map()\n",
        "Map.setCenter(-120.08, 36.86)\n",
        "bounds = Map.get_bounds()\n",
        "\n",
        "Map.addLayer(naip1, {}, \"NAIP\");\n",
        "\n",
        "Map.addLayer(water, { 'min': 0, 'max': 1, 'palette': ['aqua'], 'opacity': 0.4 }, 'water')\n",
        "\n",
        "#Add unfiltered polygons of OSM and NID dams\n",
        "Map.addLayer(ee_all, {'color': 'red', 'opacity': 0.3}, 'Polygon_dams')\n",
        "Map.addLayer(ee_polygon_GRAND, {'color': 'orange', 'opacity': 0.5}, 'Polygon_dams_GRAND')\n",
        "\n",
        "#Filtered polygons\n",
        "Map.addLayer(ee_dams_water_filter, {'color': 'blue', 'opacity': 0.6}, 'Waterfilter_dams')\n",
        "\n",
        "# Map.addLayer(ee_dams.filter(ee.Filter.eq('isInland', 1)), {'color' : 'yellow'}, 'coastline')\n",
        "\n",
        "\n",
        "Map.addLayerControl() \n",
        "Map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_U2iMYbp81Y"
      },
      "source": [
        "## Delete polygons on the coastline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4KRuHbHp7r8"
      },
      "source": [
        "ee_dams = ee_dams_water_filter.map(in_land).filter(ee.Filter.eq('isInland', 1))\n",
        "\n",
        "print(ee_dams.aggregate_array('isInland').getInfo())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb8qZWpOFSmm"
      },
      "source": [
        "dams_grand_filter = geopandas.GeoDataFrame.from_features(ee_dams.getInfo())\n",
        "overlay_polygons_2 = get_polygon_in_polygon(dams_grand_filter, buf_GRAND)\n",
        "overlay_polygons_2.drop_duplicates('FID', inplace= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_FzNYMZFb0W"
      },
      "source": [
        "ind_ = []\n",
        "for n in range(len(buf_GRAND)):\n",
        "  for i in overlay_polygons_2['FID']:\n",
        "    if int(i) == buf_GRAND['FID'][n]:\n",
        "      ind_.append(n)\n",
        "\n",
        "pol_GRAND = buf_GRAND.drop(ind_)\n",
        "\n",
        "print(pol_GRAND.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjwfQ6nqF0U-"
      },
      "source": [
        "final_dams = dams_grand_filter.append(pol_GRAND)\n",
        "final_dams.to_file('all_dams.shp')\n",
        "ee_final_dams = geemap.shp_to_ee('all_dams.shp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2LAs9trMj6c"
      },
      "source": [
        "# Displays the map\n",
        "Map = geemap.Map()\n",
        "Map.setCenter(-120.08, 36.86)\n",
        "bounds = Map.get_bounds()\n",
        "\n",
        "Map.addLayer(naip1, {}, \"NAIP\");\n",
        "\n",
        "Map.addLayer(water, { 'min': 0, 'max': 1, 'palette': ['aqua'], 'opacity': 0.4 }, 'water')\n",
        "\n",
        "#Final patches\n",
        "Map.addLayer(ee_final_dams, {'color': 'blue', 'opacity': 0.8}, 'Final_dams')\n",
        "\n",
        "Map.addLayerControl() \n",
        "Map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9UTxzIg7uyQ"
      },
      "source": [
        "# Collect negative images (No dam) dataset \n",
        "To produce the negative images (non-dam) the dataset is build using bridge locations (since are linear structures that can be mistaken for dams), lakes and random points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zTcVzEprWpN"
      },
      "source": [
        "random_points = ee.FeatureCollection.randomPoints(\n",
        "    region= control,\n",
        "    points= 200,\n",
        "    seed= 1234\n",
        ")\n",
        "\n",
        "random_points = geopandas.GeoDataFrame.from_features(random_points.getInfo())\n",
        "random_points[\"id\"] = random_points.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4Je7x1z7uyT"
      },
      "source": [
        "url = 'http://overpass-api.de/api/interpreter'  # Overpass API URL\n",
        "query = f\"\"\"\n",
        "[out:json];\n",
        "area[\"ISO3166-2\"=\"US-{state}\"][admin_level=4];\n",
        "(way[\"bridge\"=\"yes\"](area);\n",
        ");\n",
        "out center;\n",
        "\"\"\"\n",
        "\n",
        "rb = requests.get(url, params={'data': query})\n",
        "data_b = rb.json()['elements']  # read response as JSON and get the data\n",
        "df_b = json_normalize(data_b).rename(columns={\"center.lat\":\"lat\", \"center.lon\":\"lon\"})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "513bC3rIBFrW"
      },
      "source": [
        "url = 'http://overpass-api.de/api/interpreter'  # Overpass API URL\n",
        "query = f\"\"\"\n",
        "[out:json];\n",
        "area[\"ISO3166-2\"=\"US-{state}\"][admin_level=4];\n",
        "(way[\"water\"=\"lake\"](area);\n",
        " way[\"natural\"=\"water\"](area);\n",
        ");\n",
        "out center;\n",
        "\"\"\"\n",
        "\n",
        "rl = requests.get(url, params={'data': query})\n",
        "data_l = rl.json()['elements']  # read response as JSON and get the data\n",
        "df_l = json_normalize(data_l).rename(columns={\"center.lat\":\"lat\", \"center.lon\":\"lon\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BwowiFV7uyW"
      },
      "source": [
        "bridges = pd.DataFrame(df_b, columns=['lat','lon', 'id'])\n",
        "bridges.to_csv(r'osm_bridge.csv', encoding='utf-8', index=False)\n",
        "\n",
        "lakes = pd.DataFrame(df_l, columns=['lat','lon', 'id'])\n",
        "lakes.to_csv(r'osm_lake.csv', encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vosIeBs47uyX"
      },
      "source": [
        "gdf_bridge = geopandas.GeoDataFrame(bridges)\n",
        "\n",
        "#To convert the dataframe to shape to then create an earth engine object and add the layer to the map\n",
        "gdf_bridge.set_geometry(\n",
        "    geopandas.points_from_xy(gdf_bridge['lon'], gdf_bridge['lat']),\n",
        "    inplace=True, crs='EPSG:4326')\n",
        "gdf_bridge.drop(['lat', 'lon'], axis=1, inplace=True)  # optional\n",
        "gdf_bridge.to_file('bridges_osm.shp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlt4UjoiFX6Z"
      },
      "source": [
        "gdf_lake = geopandas.GeoDataFrame(lakes)\n",
        "\n",
        "#To convert the dataframe to shape to then create an earth engine object and add the layer to the map\n",
        "gdf_lake.set_geometry(\n",
        "    geopandas.points_from_xy(gdf_lake['lon'], gdf_lake['lat']),\n",
        "    inplace=True, crs='EPSG:4326')\n",
        "gdf_lake.drop(['lat', 'lon'], axis=1, inplace=True)  # optional\n",
        "gdf_lake.to_file('lakes_osm.shp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVBqJFj-CI07"
      },
      "source": [
        "random_buffer = random_points.buffer(0.01, cap_style=3)\n",
        "random_buffer = geopandas.GeoDataFrame(geometry=geopandas.GeoSeries(random_buffer))\n",
        "random_buffer['id'] = random_buffer.index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lAs0J7jCkDN"
      },
      "source": [
        "bridge_buffer = gdf_bridge.buffer(0.01, cap_style=3)\n",
        "bridge_buffer = geopandas.GeoDataFrame(geometry=geopandas.GeoSeries(bridge_buffer))\n",
        "bridge_buffer['id'] = bridge_buffer.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-gXa12pCrn4"
      },
      "source": [
        "lakes_buffer = gdf_lake.buffer(0.01, cap_style=3)\n",
        "lakes_buffer = geopandas.GeoDataFrame(geometry=geopandas.GeoSeries(lakes_buffer))\n",
        "lakes_buffer['id'] = lakes_buffer.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wrShmF9rnrj"
      },
      "source": [
        "# Refine no dam dataset\n",
        "\n",
        "To make the no-dam images more representative for the model to learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8zbY36cCNIq"
      },
      "source": [
        "all_dams = buf_OSM.append(buf_NID).append(buf_GRAND)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0ui1siTDEs0"
      },
      "source": [
        "lake_filter = get_polygon_in_polygon(lakes_buffer, all_dams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRSyMJ_5Jk0Y"
      },
      "source": [
        "lake_filter.sort_values(by=['id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HXGpaSynj9Q"
      },
      "source": [
        "if lake_filter['id'].count() == 0:\n",
        "  lakes_gdf = lakes_buffer\n",
        "else:\n",
        "  indx = []\n",
        "  for i in lake_filter['index']:\n",
        "    indx.append(i)\n",
        "  indx.sort()\n",
        "  lakes_gdf = lakes_buffer.drop(indx, axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPGAMrp0IoGO"
      },
      "source": [
        "bridge_filter = get_polygon_in_polygon(bridge_buffer, all_dams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxbwfU56v77v"
      },
      "source": [
        "bridge_filter.drop_duplicates('id', inplace=True)\n",
        "if bridge_filter['id'].count() == 0:\n",
        "  bridge_gdf = bridge_buffer\n",
        "else:\n",
        "  indx_b = []\n",
        "  for i in bridge_filter['index']:\n",
        "    indx_b.append(i)\n",
        "  indx_b.sort()\n",
        "  bridge_gdf = bridge_buffer.drop(indx_b, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlyINKOPIue7"
      },
      "source": [
        "random_filter = get_polygon_in_polygon(random_buffer, all_dams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POajgReS5Gau"
      },
      "source": [
        "if random_filter['id'].count() == 0:\n",
        "  random_gdf = random_buffer\n",
        "else:\n",
        "  indx_r = []\n",
        "  for i in random_filter['index']:\n",
        "    indx_r.append(i)\n",
        "  random_gdf = random_buffer.drop(indx_r, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyn_b0dwP8ia"
      },
      "source": [
        "lakes_gdf['FID'] = lakes_gdf.index\n",
        "bridge_gdf['FID'] = bridge_gdf.index+len(lakes_gdf)\n",
        "random_gdf['FID'] = random_gdf.index+(len(lakes_gdf)+len(bridge_gdf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqk3tDQdUlCG"
      },
      "source": [
        "lakes_gdf = lakes_gdf.sample(800, random_state=1234)\n",
        "bridge_gdf = bridge_gdf.sample(2000, random_state=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdp3nZiPUzoS"
      },
      "source": [
        "lake_n_brg = get_polygon_in_polygon(lakes_gdf, bridge_gdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbGesrAkviRf"
      },
      "source": [
        "lake_n_brg.drop_duplicates('id', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4NMs9kKusLS"
      },
      "source": [
        "id_ = []\n",
        "for i in lake_n_brg['id']:\n",
        "  id_.append(int(i))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8WgRHDF0coX"
      },
      "source": [
        "lakes_gdf.drop(id_, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o92Fn3hd3yZX"
      },
      "source": [
        "random_n_brg = get_polygon_in_polygon(bridge_gdf, random_gdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEAZ30V84CHe"
      },
      "source": [
        "idr_ = []\n",
        "for i in random_n_brg['id']:\n",
        "  idr_.append(int(i))\n",
        "\n",
        "bridge_gdf.drop(idr_, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8mF8uA6nEO0"
      },
      "source": [
        "lakes_gdf.to_file('lake_filter.shp')\n",
        "ee_lakes = geemap.shp_to_ee('lake_filter.shp')\n",
        "\n",
        "bridge_gdf.to_file('bridge_filter.shp')\n",
        "ee_bridge = geemap.shp_to_ee('bridge_filter.shp')\n",
        "\n",
        "random_gdf.to_file('random_filter.shp')\n",
        "ee_random = geemap.shp_to_ee('random_filter.shp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTCqwLhxVsno"
      },
      "source": [
        "## Filter no-dam polygons by water area"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UO3qvtUDpiT"
      },
      "source": [
        "### Now we filter the squares for which area does not reach a certain threshold\n",
        "ee_bridge_area = ee_bridge.map(compute_water_area)\n",
        "ee_lakes_area = ee_lakes.map(compute_water_area)\n",
        "\n",
        "ee_bridge_w = ee_bridge_area.filter(ee.Filter.gt('area', AreaThreshold))\n",
        "ee_lakes_w = ee_lakes_area.filter(ee.Filter.gt('area', AreaThreshold))\n",
        "\n",
        "print('All polygons OSM #{}'.format(ee_bridge.size().getInfo()))\n",
        "print('Filtered polygons OSM by area #{}'.format(ee_bridge_w.size().getInfo()))\n",
        "\n",
        "print('All polygons OSM #{}'.format(ee_lakes.size().getInfo()))\n",
        "print('Filtered polygons OSM by area #{}'.format(ee_lakes_w.size().getInfo()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKfjBp2jr7xL"
      },
      "source": [
        "# VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12evLqJc7uyj"
      },
      "source": [
        "# # Displays the map\n",
        "Map = geemap.Map()\n",
        "Map.setCenter(-120.08, 36.86)\n",
        "bounds = Map.get_bounds()\n",
        "\n",
        "Map.addLayer(naip1, {}, \"NAIP\");\n",
        "\n",
        "Map.addLayer(water, { 'min': 0, 'max': 1, 'palette': ['aqua'], 'opacity': 0.4 }, 'water')\n",
        "\n",
        "#Final patches\n",
        "Map.addLayer(ee_final_dams, {'color': 'blue', 'opacity': 0.8}, 'Final_dams')\n",
        "\n",
        "\n",
        "Map.addLayer(ee_bridge_w, {'color': 'pink', 'opacity': 0.6}, 'bridges')\n",
        "\n",
        "Map.addLayer(ee_lakes_w, {'color': 'PaleGreen', 'opacity': 0.6}, 'lakes area')\n",
        "\n",
        "Map.addLayer(ee_random, {'color': 'silver', 'opacity': 0.6}, 'random')\n",
        "\n",
        "Map.addLayerControl() \n",
        "Map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vq5RIatn1uy"
      },
      "source": [
        "# Export patches as TFRecords to Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yJR-NHchXU2"
      },
      "source": [
        "## Export dam images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBt72ne6n4Mk"
      },
      "source": [
        "# rect_list = ee_final_dams.toList(10000)\n",
        "\n",
        "# count = rect_list.size().getInfo()\n",
        "\n",
        "# SIZE = 440\n",
        "# for i in range(count):\n",
        "#   rect = ee.Feature(rect_list.get(i)).geometry()\n",
        "\n",
        "#   task = ee.batch.Export.image.toDrive(  \n",
        "#     image= naip2,\n",
        "#     description= 'PatchesExported-%d'%(i),\n",
        "#     fileNamePrefix= 'Dam-%d'%(i),\n",
        "#     folder= f'Dam_{state}',\n",
        "#     region= rect,\n",
        "#     scale= SCALE,\n",
        "#     maxPixels=10e12,\n",
        "#     fileFormat= 'TFRecord',\n",
        "#     formatOptions = {\n",
        "#       'patchDimensions': [SIZE, SIZE],\n",
        "#       'compressed': True}\n",
        "#   ).start()\n",
        "\n",
        "#   print('Export Image %d was submitted, please wait ...'%(i))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Mpamu3zGDfF"
      },
      "source": [
        "## Export non-dam images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-NuyVab8qDl"
      },
      "source": [
        "# random_list = ee_random.toList(100)\n",
        "# bridge_list = ee_bridge_w.toList(400)\n",
        "# lake_list = ee_lakes_w.toList(200)\n",
        "\n",
        "# for i in range(100):\n",
        "\n",
        "#   random = ee.Feature(random_list.get(i)).geometry()\n",
        "\n",
        "#   # // EXPORT PATCHES\n",
        "\n",
        "#   task = ee.batch.Export.image.toDrive(  \n",
        "#     image= naip2,\n",
        "#     description= 'RandomPatches-%d'%(i),\n",
        "#     fileNamePrefix= 'random-%d'%(i),\n",
        "#     scale= SCALE,\n",
        "#     folder= f'No_dam_{state}_tfr',\n",
        "#     fileFormat= 'TFRecord',\n",
        "#     region= random,\n",
        "#     formatOptions = {\n",
        "#       'patchDimensions': [SIZE, SIZE],\n",
        "#       'compressed': True}\n",
        "#   ).start()\n",
        "\n",
        "#   print('Export Random Image %d was submitted, please wait ...'%(i))\n",
        "\n",
        "\n",
        "\n",
        "# for i in range(400):\n",
        "\n",
        "#   bridges = ee.Feature(bridge_list.get(i)).geometry()\n",
        "\n",
        "#   task = ee.batch.Export.image.toDrive(  \n",
        "#     image= naip2,\n",
        "#     description= 'RandomPatches-%d'%(i),\n",
        "#     fileNamePrefix= 'bridges-%d'%(i),\n",
        "#     scale= SCALE,\n",
        "#     folder= f'No_dam_{state}_tfr',\n",
        "#     fileFormat= 'TFRecord',\n",
        "#     region= bridges,\n",
        "#     formatOptions = {\n",
        "#       'patchDimensions': [SIZE, SIZE],\n",
        "#       'compressed': True}\n",
        "#   ).start()\n",
        "\n",
        "#   print('Export Bridge Image %d was submitted, please wait ...'%(i))\n",
        "\n",
        "\n",
        "\n",
        "# for i in range(200):\n",
        "\n",
        "#   lakes = ee.Feature(lake_list.get(i)).geometry()\n",
        "\n",
        "#   task = ee.batch.Export.image.toDrive(  \n",
        "#     image= naip2,\n",
        "#     description= 'RandomPatches-%d'%(i),\n",
        "#     fileNamePrefix= 'lakes-%d'%(i),\n",
        "#     scale= SCALE,\n",
        "#     folder= f'No_dam_{state}_tfr',\n",
        "#     fileFormat= 'TFRecord',\n",
        "#     region= lakes,\n",
        "#     formatOptions = {\n",
        "#       'patchDimensions': [SIZE, SIZE],\n",
        "#       'compressed': True}\n",
        "#   ).start()\n",
        "\n",
        "#   print('Export Lake Image %d was submitted, please wait ...'%(i))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRrxCGxMt4e8"
      },
      "source": [
        "# Export patches as GeoTIFF to Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuFNFXvzt4e9"
      },
      "source": [
        "## Export dam images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XO9cLKmt4e-"
      },
      "source": [
        "# rect_list = ee_final_dams.toList(10000)\n",
        "\n",
        "# count = rect_list.size().getInfo()\n",
        "\n",
        "# SIZE = 220\n",
        "# for i in range(count):\n",
        "#   rect = ee.Feature(rect_list.get(i)).geometry()\n",
        "\n",
        "#   task = ee.batch.Export.image.toDrive(  \n",
        "#     image= naip2,\n",
        "#     description= 'DamsExported-%d'%(i),\n",
        "#     fileNamePrefix= 'Dam-%d'%(i),\n",
        "#     folder= f'Dam_{state}_tif',\n",
        "#     region= rect,\n",
        "#     scale= SCALE,\n",
        "#     maxPixels=10e12,\n",
        "#     fileFormat= 'GeoTIFF',\n",
        "\n",
        "#   ).start()\n",
        "\n",
        "#   print('Export Image %d was submitted, please wait ...'%(i))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjipzPtwt4fB"
      },
      "source": [
        "## Export non-dam images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj96qiEjt4fC"
      },
      "source": [
        "# r = 100\n",
        "# b = 350\n",
        "# l = 200\n",
        "\n",
        "# random_list = ee_random.toList(r)\n",
        "# bridge_list = ee_bridge_w.toList(b)\n",
        "# lake_list = ee_lakes_w.toList(l)\n",
        "\n",
        "# for i in range(r):\n",
        "\n",
        "#   random = ee.Feature(random_list.get(i)).geometry()\n",
        "\n",
        "#   # // EXPORT PATCHES\n",
        "\n",
        "#   task = ee.batch.Export.image.toDrive(  \n",
        "#     image= naip2,\n",
        "#     description= 'RandomPatches-%d'%(i),\n",
        "#     fileNamePrefix= 'random-%d'%(i),\n",
        "#     scale= SCALE,\n",
        "#     folder= f'No_dam_{state}_tif',\n",
        "#     fileFormat= 'GeoTIFF',\n",
        "#     region= random,\n",
        "\n",
        "#   ).start()\n",
        "\n",
        "#   print('Export Random Image %d was submitted, please wait ...'%(i))\n",
        "\n",
        "\n",
        "\n",
        "# for i in range(b):\n",
        "\n",
        "#   bridges = ee.Feature(bridge_list.get(i)).geometry()\n",
        "\n",
        "#   task = ee.batch.Export.image.toDrive(  \n",
        "#     image= naip2,\n",
        "#     description= 'RandomPatches-%d'%(i),\n",
        "#     fileNamePrefix= 'bridges-%d'%(i),\n",
        "#     scale= SCALE,\n",
        "#     folder= f'No_dam_{state}_tif',\n",
        "#     fileFormat= 'GeoTIFF',\n",
        "#     region= bridges,\n",
        "\n",
        "#   ).start()\n",
        "\n",
        "#   print('Export Bridge Image %d was submitted, please wait ...'%(i))\n",
        "\n",
        "\n",
        "\n",
        "# for i in range(l):\n",
        "\n",
        "#   lakes = ee.Feature(lake_list.get(i)).geometry()\n",
        "\n",
        "#   task = ee.batch.Export.image.toDrive(  \n",
        "#     image= naip2,\n",
        "#     description= 'RandomPatches-%d'%(i),\n",
        "#     fileNamePrefix= 'lakes-%d'%(i),\n",
        "#     scale= SCALE,\n",
        "#     folder= f'No_dam_{state}_tif',\n",
        "#     fileFormat= 'GeoTIFF',\n",
        "#     region= lakes,\n",
        "\n",
        "#   ).start()\n",
        "\n",
        "#   print('Export Lake Image %d was submitted, please wait ...'%(i))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}